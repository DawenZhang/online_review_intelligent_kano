{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "customer_review_extraction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DawenZhang/online_review_intelligent_kano/blob/filled/product_review_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMwVe9R5YDXr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown # webpage preparation\n",
        "\n",
        "#@markdown the webpage zip file path; if you wish to upload file, leave it blank\n",
        "webpage_zip_path = \"https://github.com/DawenZhang/online_review_intelligent_kano/raw/filled/provided/product_2/review_pages.zip\" #@param {type:\"string\"}\n",
        "\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "file_path = \"\"\n",
        "if webpage_zip_path == \"\":\n",
        "  uploaded = {}\n",
        "  while len([*uploaded.keys()]) <= 0:\n",
        "    print(\"as the path field is left blank, please upload webpage zip file\")\n",
        "    uploaded = files.upload()\n",
        "  file_path = [*uploaded.keys()][0]\n",
        "else:\n",
        "  import urllib.request\n",
        "  file_path = urllib.request.urlretrieve(webpage_zip_path)[0]\n",
        "\n",
        "print('webpage received')\n",
        "zip_ref = zipfile.ZipFile(file_path, 'r')\n",
        "zip_ref.extractall('webpage/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD-KeIWZgeiW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "\n",
        "#@markdown the folder containing webpage files, no ending slash\n",
        "folder_name = \"review_pages\" #@param {type:\"string\"}\n",
        "\n",
        "webpage_names = os.listdir('webpage/' + folder_name + '/')\n",
        "\n",
        "dataset = []\n",
        "\n",
        "comment_count = 1\n",
        "for webpage_name in webpage_names:\n",
        "  with open('webpage/' + folder_name + '/' + webpage_name, 'r', encoding = 'utf-8') as webpage_file:\n",
        "    start = 0\n",
        "    end = 0\n",
        "    review_id = None\n",
        "    product_id = None\n",
        "    review_content = None\n",
        "    rating = None\n",
        "    date = None\n",
        "    helpful_vote = None\n",
        "    verified_purchase = None\n",
        "    top_reviewer = None\n",
        "    title = None\n",
        "    bought_model = None\n",
        "    is_anomaly = None\n",
        "    \n",
        "    webpage_line = webpage_file.readline()\n",
        "    while webpage_line:\n",
        "      review_id = comment_count\n",
        "      product_id = 1\n",
        "      webline = webpage_line.strip()\n",
        "      p = re.compile('cm_cr-review_list')\n",
        "      m = p.search(webline)\n",
        "      if m != None:\n",
        "        start = 1\n",
        "        end = 0\n",
        "      if start == 1 and end == 0:\n",
        "        p = re.compile('a-form-actions a-spacing-top-extra-large')\n",
        "        m = p.search(webline)\n",
        "        if m != None:\n",
        "          end = 1\n",
        "        else:\n",
        "          p = re.compile('<i data-hook=\"review-star-rating\" class=\"a-icon a-icon-star a-star-([0-5]*?) review-rating\">')\n",
        "          m = p.search(webline)\n",
        "          if m != None:\n",
        "            rating = int(m.group(1))\n",
        "\n",
        "          p = re.compile('<a data-hook=\"review-title\" class=\"a-size-base a-link-normal review-title a-color-base a-text-bold\" href=\".*?\">(.*?)</a>')\n",
        "          m = p.search(webline)\n",
        "          if m != None:\n",
        "            title = m.group(1)\n",
        "\n",
        "          p = re.compile('badge-top-1000-reviewer')\n",
        "          m = p.search(webline)\n",
        "          if m != None:\n",
        "            top_reviewer = True\n",
        "            \n",
        "          p = re.compile('class=\"a-size-mini a-link-normal a-color-secondary\" href=\".*?\">(.*?)</a>')\n",
        "          m = p.search(webline)\n",
        "          if m != None:\n",
        "            bought_model = m.group(1)\n",
        "            \n",
        "          p = re.compile('class=\"a-size-mini a-color-state a-text-bold\">Verified Purchase</span>')\n",
        "          m = p.search(webline)\n",
        "          if m != None:\n",
        "            verified_purchase = True\n",
        "            \n",
        "          p = re.compile('<span data-hook=\"review-body\" class=\"a-size-base review-text\">(.*?)</span></div>')\n",
        "          m = p.search(webline)\n",
        "          if m != None:\n",
        "            review_content = m.group(1)\n",
        "            \n",
        "          p = re.compile('<span data-hook=\"helpful-vote-statement\" class=\"a-size-base a-color-tertiary cr-vote-text\">(.*?) people found this helpful</span>')\n",
        "          m = p.search(webline)\n",
        "          if m != None:\n",
        "            helpful_vote = int(m.group(1).replace(',', ''))\n",
        "            \n",
        "          p = re.compile('<span data-hook=\"review-date\" class=\"a-size-base a-color-secondary review-date\">(.*?)</span>')\n",
        "          m = p.search(webline)\n",
        "          if m != None:\n",
        "            date = datetime.strptime(m.group(1), '%B %d, %Y')\n",
        "          \n",
        "          p = re.compile('cr-footer-line-height')\n",
        "          m = p.search(webline)\n",
        "          if m != None:\n",
        "            dataset.append((review_id, product_id, review_content, rating, date, helpful_vote, verified_purchase, top_reviewer, title, bought_model, is_anomaly))\n",
        "            review_id = None\n",
        "            product_id = None\n",
        "            review_content = None\n",
        "            rating = None\n",
        "            date = None\n",
        "            helpful_vote = None\n",
        "            verified_purchase = None\n",
        "            top_reviewer = None\n",
        "            title = None\n",
        "            bought_model = None\n",
        "            is_anomaly = None\n",
        "            comment_count += 1\n",
        "            \n",
        "      if start == 1 and end == 1:\n",
        "        break\n",
        "      webpage_line = webpage_file.readline()\n",
        "    webpage_file.close()\n",
        "    \n",
        "dataframe = pd.DataFrame(data = dataset, columns = ['review_id', 'product_id', 'review_content', 'rating', 'date', 'helpful_vote', 'verified_purchase', 'top_reviewer', 'title', 'bought_model', 'is_anomaly'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqpbM5CvtaAN",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "#@markdown google nlp credential url; if you wish to upload file, leave it blank\n",
        "#@markdown if no credential owned, please generate one from Google Cloud Console \n",
        "#@markdown **it is strongly unrecommended to expose your credential file on the web**\n",
        "google_nlp_credential_web_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "credential_file_path = \"\"\n",
        "if google_nlp_credential_web_path == \"\":\n",
        "  from google.colab import files\n",
        "  uploaded = {}\n",
        "  while len([*uploaded.keys()]) <= 0:\n",
        "    print(\"as the path field is left blank, please upload credential file\")\n",
        "    uploaded = files.upload()\n",
        "  credential_file_path = [*uploaded.keys()][0]\n",
        "else:\n",
        "  import urllib.request\n",
        "  credential_file_path = urllib.request.urlretrieve(google_nlp_credential_web_path)[0]\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_file_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qetAaJvxpTEm",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown # Google NLP Analysis\n",
        "#@markdown run to process\n",
        "\n",
        "entity_dataset = []\n",
        "mention_dataset = []\n",
        "document_dataset = []\n",
        "sentence_dataset = []\n",
        "error_reviews = []\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "from time import sleep\n",
        "\n",
        "from google.cloud import language\n",
        "from google.cloud.language import enums\n",
        "from google.cloud.language import types\n",
        "from google.protobuf import json_format\n",
        "from google.cloud.language_v1.types import AnalyzeEntitySentimentResponse\n",
        "from google.cloud.language_v1.types import AnalyzeSentimentResponse\n",
        "\n",
        "entity_type = ('UNKNOWN', 'PERSON', 'LOCATION', 'ORGANIZATION', 'EVENT', 'WORK_OF_ART', 'CONSUMER_GOOD', 'OTHER')\n",
        "mention_type = ('TYPE_UNKNOWN', 'PROPER', 'COMMON')\n",
        "\n",
        "client = language.LanguageServiceClient()\n",
        "\n",
        "meta_count = 0\n",
        "total_count = len(dataset)\n",
        "\n",
        "for meta_review in dataset:\n",
        "  \n",
        "  if meta_count % round(total_count / 10) == 0:\n",
        "    print(\"analyzing: \", str(meta_count), \"/\", str(total_count))\n",
        "  meta_count += 1\n",
        "  \n",
        "  sleep(0.2)\n",
        "  document = types.Document(\n",
        "      content = meta_review[2].encode('utf-8'),\n",
        "      type = enums.Document.Type.PLAIN_TEXT\n",
        "  )\n",
        "\n",
        "  encoding = enums.EncodingType.UTF32\n",
        "  if sys.maxunicode == 65535:\n",
        "    encoding = enums.EncodingType.UTF16\n",
        "  \n",
        "  try:\n",
        "    entity_result = client.analyze_entity_sentiment(document, encoding)\n",
        "    \n",
        "    for entity_num_from_0 in range(len(entity_result.entities)):\n",
        "      entity_id = entity_num_from_0 + 1\n",
        "      entity_dataset.append((meta_review[0], meta_review[1], entity_id, entity_result.entities[entity_num_from_0].salience, entity_result.entities[entity_num_from_0].sentiment.score, entity_result.entities[entity_num_from_0].sentiment.magnitude, entity_result.entities[entity_num_from_0].name, entity_type[entity_result.entities[entity_num_from_0].type]))\n",
        "      \n",
        "      for mention in entity_result.entities[entity_num_from_0].mentions:\n",
        "        mention_dataset.append((meta_review[0], meta_review[1], entity_id, mention.text.begin_offset, mention.sentiment.score, mention.sentiment.magnitude, mention.text.content, mention_type[mention.type]))\n",
        "    \n",
        "  except Exception as e:\n",
        "    error_reviews.append((meta_review[0], meta_review[1]))\n",
        "  \n",
        "  try:\n",
        "    document_result = client.analyze_sentiment(document, encoding)\n",
        "    document_dataset.append((meta_review[0], meta_review[1], document_result.document_sentiment.score, document_result.document_sentiment.magnitude))\n",
        "    \n",
        "    for sentence in document_result.sentences:\n",
        "      sentence_dataset.append((meta_review[0], meta_review[1], sentence.text.begin_offset, sentence.text.content, sentence.sentiment.score, sentence.sentiment.magnitude))\n",
        "  except Exception as e:\n",
        "    error_reviews.append((meta_review[0], meta_review[1]))\n",
        "\n",
        "print('the following reviews(review_id, product_id) have got errors when being processed by Google Cloud NLP: ', error_reviews)\n",
        "\n",
        "entity_dataframe = pd.DataFrame(data = entity_dataset, columns = ['review_id', 'product_id', 'entity_id', 'salience', 'sentiment_score', 'sentiment_magnitude', 'name', 'type'])\n",
        "mention_dataframe = pd.DataFrame(data = mention_dataset, columns = ['review_id', 'product_id', 'entity_id', 'begin_offset', 'sentiment_score', 'sentiment_magnitude', 'content', 'type'])\n",
        "sentence_dataframe = pd.DataFrame(data = sentence_dataset, columns = ['review_id', 'product_id', 'begin_offset', 'content', 'sentiment_score', 'sentiment_magnitude'])\n",
        "document_dataframe = pd.DataFrame(data = document_dataset, columns = ['review_id', 'product_id', 'sentiment_score', 'sentiment_magnitude'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--1ZqMVp-5QW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown tick and run to download datasheet\n",
        "download_datasheet = True #@param {type:\"boolean\"}\n",
        "if download_datasheet == True:\n",
        "  \n",
        "  from google.colab import files\n",
        "  \n",
        "  dataframe.to_csv(\"customer_reviews.csv\", index = False, header = True)\n",
        "  files.download('customer_reviews.csv')\n",
        "  \n",
        "  entity_dataframe.to_csv('entity_sentiment.csv',index=False,header=True)\n",
        "  files.download('entity_sentiment.csv')\n",
        "  \n",
        "  mention_dataframe.to_csv('mention_sentiment.csv',index=False,header=True)\n",
        "  files.download('mention_sentiment.csv')\n",
        "  \n",
        "  sentence_dataframe.to_csv('sentence_sentiment.csv',index=False,header=True)\n",
        "  files.download('sentence_sentiment.csv')\n",
        "  \n",
        "  document_dataframe.to_csv('document_sentiment.csv',index=False,header=True)\n",
        "  files.download('document_sentiment.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMXJDoduSGFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}